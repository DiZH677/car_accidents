{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.dataframe\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/29 11:33:19 WARN Utils: Your hostname, MacBook-Pro-M.local resolves to a loopback address: 127.0.0.1; using 192.168.1.117 instead (on interface en0)\n",
      "24/01/29 11:33:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "24/01/29 11:33:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "def init_spark():\n",
    "    return (\n",
    "        SparkSession.builder.master(\"local[*]\")\n",
    "        .appName(\"PySpark_Working\")\n",
    "        .config(\n",
    "            \"spark.jars\",\n",
    "            \"/Users/dmitrijzigunov/Library/Application Support/JetBrains/DataGrip2023.3/jdbc-drivers/PostgreSQL/42.6.0/org/postgresql/postgresql/42.6.0/postgresql-42.6.0.jar\",\n",
    "        )\n",
    "        .getOrCreate()\n",
    "    )\n",
    "\n",
    "spark = init_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dtp_spark(sesspark, path_name, is_print = False) -> pyspark.sql.dataframe.DataFrame:\n",
    "    uchInfo = StructType([\n",
    "        StructField(\"ALCO\", StringType(), True),\n",
    "        StructField(\"k_UCH\", StringType(), True),\n",
    "        StructField(\"NPDD\", StringType(), True),\n",
    "        StructField(\"n_UCH\", StringType(), True),\n",
    "        StructField(\"POL\", StringType(), True),\n",
    "        StructField(\"SOP_NPDD\", StringType(), True),\n",
    "        StructField(\"s_SM\", StringType(), True),\n",
    "        StructField(\"s_T\", StringType(), True),\n",
    "        StructField(\"v_ST\", StringType(), True),\n",
    "    ])\n",
    "    driverSchema = StructType([\n",
    "        StructField(\"ALCO\", StringType(), True),\n",
    "        StructField(\"INJURED_CARD_ID\", StringType(), True),\n",
    "        StructField(\"k_UCH\", StringType(), True),\n",
    "        StructField(\"NPDD\", StringType(), True),\n",
    "        StructField(\"n_UCH\", StringType(), True),\n",
    "        StructField(\"POL\", StringType(), True),\n",
    "        StructField(\"SAFETY_BELT\", StringType(), True),\n",
    "        StructField(\"SOP_NPDD\", StringType(), True),\n",
    "        StructField(\"s_SEAT_GROUP\", StringType(), True),\n",
    "        StructField(\"s_SM\", StringType(), True),\n",
    "        StructField(\"s_T\", StringType(), True),\n",
    "        StructField(\"v_ST\", IntegerType(), True),\n",
    "    ])\n",
    "    tsInfoSchema = StructType([\n",
    "        StructField(\"color\", StringType(), True),\n",
    "        StructField(\"f_sob\", StringType(), True),\n",
    "        StructField(\"g_v\", StringType(), True),\n",
    "        StructField(\"m_pov\", StringType(), True),\n",
    "        StructField(\"m_ts\", StringType(), True),\n",
    "        StructField(\"marka_ts\", StringType(), True),\n",
    "        StructField(\"n_ts\", StringType(), True),\n",
    "        StructField(\"o_pf\", StringType(), True),\n",
    "        StructField(\"r_rul\", StringType(), True),\n",
    "        StructField(\"t_n\", StringType(), True),\n",
    "        StructField(\"t_ts\", StringType(), True),\n",
    "        StructField(\"ts_s\", StringType(), True),\n",
    "        StructField(\"ts_uch\", ArrayType(driverSchema), True),\n",
    "    ])\n",
    "\n",
    "    infoDtpSchema = StructType([\n",
    "        StructField(\"CHOM\", StringType(), True),\n",
    "        StructField(\"COORD_L\", StringType(), True),\n",
    "        StructField(\"COORD_W\", StringType(), True),\n",
    "        StructField(\"dor\", StringType(), True),\n",
    "        StructField(\"dor_k\", StringType(), True),\n",
    "        StructField(\"dor_z\", StringType(), True),\n",
    "        StructField(\"factor\", StringType(), True),\n",
    "        StructField(\"house\", StringType(), True),\n",
    "        StructField(\"k_ul\", StringType(), True),\n",
    "        StructField(\"km\", StringType(), True),\n",
    "        StructField(\"m\", StringType(), True),\n",
    "        StructField(\"NP\", StringType(), True),\n",
    "        StructField(\"ndu\", StringType(), True),\n",
    "        StructField(\"OBJ_DTP\", StringType(), True),\n",
    "        StructField(\"osv\", StringType(), True),\n",
    "        StructField(\"s_dtp\", StringType(), True),\n",
    "        StructField(\"s_pch\", StringType(), True),\n",
    "        StructField(\"sdor\", StringType(), True),\n",
    "        StructField(\"spog\", StringType(), True),\n",
    "        StructField(\"street\", StringType(), True),\n",
    "        StructField(\"ts_info\", ArrayType(tsInfoSchema), True),\n",
    "        StructField(\"uchInfo\", ArrayType(uchInfo), True),\n",
    "    ])\n",
    "\n",
    "    tabSchema = StructType([\n",
    "        StructField(\"DTPV\", StringType(), True),\n",
    "        StructField(\"date\", StringType(), True),\n",
    "        StructField(\"district\", StringType(), True),\n",
    "        StructField(\"EMTP_NUMBER\", LongType(), True),\n",
    "        StructField(\"infoDtp\", infoDtpSchema, True),\n",
    "        StructField(\"KTS\", StringType(), True),\n",
    "        StructField(\"KUCH\", StringType(), True),\n",
    "        StructField(\"kartId\", StringType(), True),\n",
    "        StructField(\"POG\", StringType(), True),\n",
    "        StructField(\"RAN\", StringType(), True),\n",
    "        StructField(\"rowNum\", StringType(), True),\n",
    "        StructField(\"time\", StringType(), True),\n",
    "    ])\n",
    "    \n",
    "\n",
    "    df = sesspark.read.option(\"multiline\", \"true\").json(\n",
    "        path_name,\n",
    "        schema=tabSchema\n",
    "    )\n",
    "    if is_print:\n",
    "        df.printSchema()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Работа с подключением к Postgres и загрузка таблиц с вычислением их идентификатором"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_table(db_table, user_name=\"dmitrijzigunov\", url_ct=\"jdbc:postgresql://localhost:5432/car_accidents\"):\n",
    "    return (\n",
    "        spark.read.format(\"jdbc\")\n",
    "        .options(\n",
    "            url=url_ct,\n",
    "            dbtable=db_table,\n",
    "            user=user_name,\n",
    "            driver=\"org.postgresql.Driver\",\n",
    "        )\n",
    "        .load()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Добавление и согласование идентификаторов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Можно вызвать selectExpr, и тогда названия колонок просто перечислять через запятую \n",
    "\n",
    "# Добавление столбца с идентификаторами\n",
    "def load_dtp_data(main_frame, id_function, is_print=False):\n",
    "    dtp_data_loading = main_frame.select(\n",
    "        col(\"DTPV\").cast(StringType()).alias(\"description\"), \n",
    "        to_timestamp(concat(col(\"date\"), lit(\" \"), col(\"time\")), \"dd.MM.yyyy HH:mm\").alias(\"datetime\"),\n",
    "        col(\"time\").cast(StringType()), \n",
    "        col(\"infoDtp.COORD_L\").cast(FloatType()).alias(\"coord_l\"),\n",
    "        col(\"infoDtp.COORD_W\").cast(FloatType()).alias(\"coord_w\"),\n",
    "        col(\"infoDtp.dor\").cast(StringType()).alias(\"dor\"), \n",
    "        col(\"infoDtp.osv\").cast(StringType()).alias(\"osv\"), \n",
    "        col(\"KTS\").cast(IntegerType()).alias(\"count_ts\"), \n",
    "        col(\"KUCH\").cast(IntegerType()).alias(\"count_parts\")\n",
    "    ).withColumn(\"ID\", id_function(monotonically_increasing_id(), lit(\"dtp\")))\n",
    "    if is_print:\n",
    "        print(dtp_data_loading.show(n=5))\n",
    "    return dtp_data_loading\n",
    "# Добавление столбца с идентификаторами\n",
    "def load_vehicle_data(main_frame, id_function, is_print=False):\n",
    "    vehicle_data_loading = main_frame.withColumn(\"ID\", id_function(monotonically_increasing_id(), lit(\"dtp\")))\\\n",
    "    .select(\"id\", explode(col(\"infoDtp.ts_info\")).alias(\"ts_info\"))\\\n",
    "    .select(\n",
    "    col(\"ts_info.marka_ts\"),\n",
    "    col(\"ts_info.m_ts\"),\n",
    "    col(\"ts_info.r_rul\"),\n",
    "    col(\"ts_info.t_ts\").alias(\"type_ts\"),\n",
    "    col(\"ts_info.g_v\").cast(IntegerType()).alias(\"car_year\"),\n",
    "    col(\"ts_info.color\"),\n",
    "    col(\"ID\").alias(\"dtp_id\")\n",
    ").withColumn(\"ID\", id_function(monotonically_increasing_id(), lit(\"vehicle\")))\n",
    "    if is_print:\n",
    "        print(vehicle_data_loading.show(n=8))\n",
    "    return vehicle_data_loading\n",
    "# Добавление столбца с идентификаторами\n",
    "def load_participant_data(main_frame, id_function, is_print=False):\n",
    "    partic_data_loading = main_frame.select(explode(col(\"infoDtp.ts_info\")).alias(\"ts_info\"))\\\n",
    "    .withColumn(\"vehicle_id\", id_function(monotonically_increasing_id(), lit(\"vehicle\")))\\\n",
    "    .select(\"vehicle_id\", explode(col(\"ts_info.ts_uch\")).alias(\"participant\")).select(\n",
    "    col('participant.k_UCH').alias(\"category\"),\n",
    "    col('participant.NPDD').alias(\"warnings\"),\n",
    "    col('participant.SAFETY_BELT').alias(\"safety_belt\"),\n",
    "    col('participant.POL').alias(\"pol\"),\n",
    "    col('participant.s_T').alias(\"health\"),\n",
    "    col('vehicle_id')).withColumn(\"safety_belt\", when(col(\"safety_belt\") == \"Да\", True).otherwise(False))\\\n",
    "    .withColumn(\"ID\", id_function(monotonically_increasing_id(), lit(\"participant\")))\n",
    "    if is_print:\n",
    "        print(partic_data_loading.show(n=50))\n",
    "    return partic_data_loading\n",
    "\n",
    "# dtp_data = load_dtp_data(dtp, generate_id_udf, True)\n",
    "# vehicle_data = load_vehicle_data(dtp, generate_id_udf, True)\n",
    "# partic_data = load_participant_data(dtp, generate_id_udf, True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка в Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dtp_columns():\n",
    "    return [\"id\", \"description\", \"datetime\", \"coord_w\", \"coord_l\", \"dor\", \"osv\", \"count_ts\", \"count_parts\"]\n",
    "\n",
    "def get_vehicle_columns():\n",
    "    return [\"id\", \"dtp_id\", \"marka_ts\", \"m_ts\", \"r_rul\", \"type_ts\", \"car_year\", \"color\"]\n",
    "\n",
    "def get_participant_columns():\n",
    "    return [\"id\", \"vehicle_id\", \"category\", \"warnings\", \"safety_belt\", \"pol\", \"health\"]\n",
    "\n",
    "\n",
    "# Настройка подключения к Postgres и чтение таблицы\n",
    "def save_table(df, db_table, mode=\"append\", user_name=\"dmitrijzigunov\", url_ct=\"jdbc:postgresql://localhost:5432/car_accidents\"):\n",
    "    if db_table == \"dtp\":\n",
    "        df = df.select(get_dtp_columns())\n",
    "    elif db_table == \"vehicle\":\n",
    "        df = df.select(get_vehicle_columns())\n",
    "    elif db_table == \"participant\":\n",
    "        df = df.select(get_participant_columns())\n",
    "    df.write.format(\"jdbc\")\\\n",
    "    .options(\n",
    "        url=url_ct,\n",
    "        dbtable=db_table,\n",
    "        user=user_name,\n",
    "        driver=\"org.postgresql.Driver\",\n",
    "    )\\\n",
    "    .mode(mode)\\\n",
    "    .save()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Полный цикл ETL\n",
    "\n",
    "Чтобы узнать адрес, по которому нужно обращаться к hdfs, открыть файл \n",
    "/opt/homebrew/Cellar/hadoop/3.3.6/libexec/etc/hadoop/core-site.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_save(file_name, is_print=False):\n",
    "    if is_print:\n",
    "        print(f\"Loading {file_name}...\")\n",
    "    # path_file = \"hdfs://localhost:9000/data/November2022.json\"\n",
    "    path_file = file_name\n",
    "    textFile = spark.read.option(\"multiline\",\"true\").json(path_file)\n",
    "    \n",
    "    dtp = read_dtp_spark(spark, path_file)\n",
    "    \n",
    "    next_id_dtp = load_table(\"dtp\").selectExpr(\"max(id) as max_id\").first()[\"max_id\"]\n",
    "    next_id_veh = load_table(\"vehicle\").selectExpr(\"max(id) as max_id\").first()[\"max_id\"]\n",
    "    next_id_par = load_table(\"participant\").selectExpr(\"max(id) as max_id\").first()[\"max_id\"]\n",
    "    if is_print:\n",
    "        print(f\"Next id key – {next_id_dtp} dtp, {next_id_veh} vehicle, {next_id_par} participant\")\n",
    "    next_ids = {\n",
    "        \"dtp\": next_id_dtp,\n",
    "        \"vehicle\": next_id_veh,\n",
    "        \"participant\": next_id_par\n",
    "    }\n",
    "    # Определение пользовательской функции\n",
    "    def generate_id(row_num, table_name):\n",
    "        next_id = next_ids.get(table_name, 0)\n",
    "        if next_id is None:\n",
    "            next_id = 0\n",
    "        else:\n",
    "            next_id += 1\n",
    "        return row_num + next_id\n",
    "    # Регистрация пользовательской функции\n",
    "    generate_id_udf = udf(generate_id, IntegerType())\n",
    "    \n",
    "    dtp_data = load_dtp_data(dtp, generate_id_udf)\n",
    "    vehicle_data = load_vehicle_data(dtp, generate_id_udf)\n",
    "    partic_data = load_participant_data(dtp, generate_id_udf)\n",
    "    \n",
    "    if is_print:\n",
    "        print(f\"Loading dtp...\\n{dtp_data.show(n=5)}\")\n",
    "        print(f\"Loading veh...\\n{vehicle_data.show(n=5)}\")\n",
    "        print(f\"Loading par...\\n{partic_data.show(n=5)}\")\n",
    "    \n",
    "    save_table(dtp_data, \"dtp\")\n",
    "    save_table(vehicle_data, \"vehicle\")\n",
    "    save_table(partic_data, \"participant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Рабочая область\n",
    "#### При необходимости комментировать уже загруженные строки\n",
    "#### Чтобы узнать адрес, по которому нужно обращаться к hdfs, открыть файл  /opt/homebrew/Cellar/hadoop/3.3.6/libexec/etc/hadoop/core-site.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_and_save(\"hdfs://localhost:9000/data/November2022.json\", False)\n",
    "load_and_save(\"hdfs://localhost:9000/data/December2022.json\", False)\n",
    "load_and_save(\"hdfs://localhost:9000/data/January2023.json\", False)\n",
    "load_and_save(\"hdfs://localhost:9000/data/February2023.json\", False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
